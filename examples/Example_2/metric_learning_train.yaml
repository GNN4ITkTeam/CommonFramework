# CommonFramework configuration
stage: graph_construction
model: MetricLearning
input_dir: /mnt/data0/dittmeier/data/gnn4itk/feature_store/ # Should contain the files used in training and inference
stage_dir: /mnt/data0/dittmeier/data/gnn4itk/QAT_MetricLearning_ITK/ # A directory to be created that will store logs, artifacts, and output data
project: CF_QAT_MetricLearning_ITK # Used in logging
gpus: 1
nodes: 1
num_workers: [0, 0]

# Dataset parameters
data_split: [80, 10, 10] # Number of [training, validation, testing] examples

# Truth and weighting parameters. The syntax is as follows:
  # Single value: track[key] == value
  # List of 2 floats: value1 <= track[key] <= value2
  # List with a string as the first value: First value defines the boolean test, the second value is a list or value. Boolean tests are: (in, not_in, is, is_not, within, not_within)
  # All other tracks follow the default: If a true edge, weight=0. If a fake edge, weight=1.

  # NOTE: let parameter sweep find good configuration? evaluated signal purity at 98% signal efficiency?
weighting:
  - weight: 1.
    conditions:
      y: False
  - weight: 0.
    conditions:
      y: True
  - weight: 3.
    conditions:
      y: True
      pt: [500, .inf]
      nhits: [3, .inf]
      primary: True
      pdgId: [not_in, [11, -11]]

# A list of hard cut conditions. Any hits not passing these conditions will be removed from the training and validation dataset.
# By definition, noise will not pass any of these cuts, so any hard cuts will remove noise.
hard_cuts:
  pt: [1000, .inf]

# Model parameters
undirected: True
node_features: [r, phi, z]
node_scales: [1000, 3.14, 1000]
emb_hidden: 512
nb_layer: 4
emb_dim: 12
activation: ReLU
randomisation: 1
points_per_batch: 50000
r_train: 0.1
knn: 50
knn_val: 500
knn_test: 100
norm: True # not yet supported for quantized network

# Training parameters
warmup: 5
margin: 0.1
lr: 0.01
factor: 0.7
patience: 10
max_epochs: 150
# define metric for best model, checkpoint will save that and last model
metric_to_monitor: final_bops # smallest model that fulfills our purity requirement
metric_mode: min # minimize the final_bops
# final_bops requires in addition: config to measure purity at fixed efficiency!
log_working_metrics: True # purity at 98 % efficiency
target_purity: 0.14 # 0.006  # reference value from unquantized network
onnx_export: True # config to export to QONNX and evaluate BOPS

# QNN Parameters
quantized_network: True # if False, uses standard PyTorch MLP
weight_bit_width_input: 8
weight_bit_width_hidden: 8
weight_bit_width_output: 8
bias: True
bias_quant: 8  # allowed settings: 8, 16, 24, 32 (defaults to 32 otherwise)
activation_qnn: True # telling the network to use quantReLU(); required for bias quantization!!
activation_bit_width_input: 8
activation_bit_width_hidden: 8
activation_bit_width_output: 8
output_activation: False # adds an activation layer at the end
output_activation_quantization: False # returns QuantTensor if True
batch_norm: True # comes for free with FINN
layer_norm: False # layerNorm not yet supported by QONNX

# Input quantization Parameters, only using fixed point representation; also possible with standard PyTorch MLP!
input_quantization: True # also enables QuantIdentity
integer_part: 3 # fixed point representation: < sign + int . frac >
fractional_part: 20

# pruning
pruning_allow: True
pruning_val_loss: 0.000002 # if validation loss is within this range for 10 epochs, prune
pruning_fn: "l1_unstructured"
pruning_amount: 0.5
pruning_freq: 14 # prune first time only after this number of epochs, then val_loss can also lead to pruning
rewind_lr: True # for rewinding learning rate after pruning
l1_loss: False # penalize large weights
l1_factor: 0.000005 # set appropriate value wrt actual training loss
