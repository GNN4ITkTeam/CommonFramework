stage: edge_classifier
model: InteractionGNN
dataset_class: GraphDataset
input_dir: /global/cfs/cdirs/m3443/data/GNN4ITK/CommonFrameworkExamples/chep_2022_pipeline/filter # Should contain the files used in training and inference
stage_dir: /global/cfs/cdirs/m3443/data/GNN4ITK/CommonFrameworkExamples/chep_2022_pipeline/gnn # A directory to be created that will store logs, artifacts, and output data
project: CF_CHEP2022_HomoGNN_Uncorrelated # Used in logging
group: homognn_undirected_cluster
gpus: 4
nodes: 1

# Dataset parameters
data_split: [3842, 162, 180] #[10, 5, 5] # [400, 50, 50] # Number of [training, validation, testing] examples
input_cut: 0.1

# Truth and weighting parameters. Syntax is...
# Single value: track[key] == value
# List of 2 floats: value1 <= track[key] <= value2
# List with a string as the first value: First value defines the boolean test, the second value is a list or value. Boolean tests are: (in, not_in, is, is_not, within, not_within)
# All other tracks follow the default: If a true edge, weight=0. If a fake edge, weight=1.
weighting:
  - weight: 1.
    conditions: 
      y: False
  - weight: 0.
    conditions:
      y: True
  - weight: 3.
    conditions:
      y: True
      pt: [1000., .inf]
      nhits: [3, .inf]
      primary: True
      pdgId: [not_in, [11, -11]]

# A list of hard cut conditions. Any hits not passing these conditions will be removed from the training and validation dataset.
# By definition, noise will not pass any of these cuts, so any hard cuts will remove noise.
hard_cuts:

# Model parameters
edge_cut: 0.5
undirected: true
node_features: [r, phi, z, count_1, charge_count_1, count_2, charge_count_2, cluster_x_1, cluster_y_1, cluster_z_1, cluster_x_2, cluster_y_2, cluster_z_2]
node_scales:   [1000, 3.14, 1000, 1, 1, 1, 1, 1000, 1000, 1000, 1000, 1000, 1000]
hidden: 64
n_graph_iters: 8
nb_node_layer: 3
nb_edge_layer: 3
layernorm: True
batchnorm: False
aggregation: [sum, mean]
hidden_activation: SiLU
output_activation: SiLU
node_net_recurrent: False
edge_net_recurrent: False

# Training parameters
warmup: 10
lr: 0.001
min_lr: 0.00005
factor: 0.9
patience: 20
max_epochs: 1000
debug: false
num_workers: [8,8,8]