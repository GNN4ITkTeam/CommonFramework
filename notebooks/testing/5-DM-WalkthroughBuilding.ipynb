{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough Method Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Test the weighting and hard cut config of the data loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/gnn4itk/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/GNN4ITK/CommonFrameworkExamples/Example_1_Dev/gnn/valset/\"\n",
    "input_files = os.listdir(input_dir)\n",
    "input_files = [os.path.join(input_dir, f) for f in input_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.load(input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(hit_id=[278183], x=[278183], y=[34671], z=[278183], r=[278183], phi=[278183], eta=[278183], region=[278183], cluster_x_1=[278183], cluster_y_1=[278183], cluster_z_1=[278183], cluster_x_2=[278183], cluster_y_2=[278183], cluster_z_2=[278183], norm_x=[278183], norm_y=[278183], norm_z_1=[278183], eta_angle_1=[278183], phi_angle_1=[278183], eta_angle_2=[278183], phi_angle_2=[278183], norm_z_2=[278183], track_edges=[2, 14704], particle_id=[14704], pt=[14704], radius=[14704], primary=[14704], nhits=[14704], pdgId=[14704], config=[2], event_id='000000102', edge_index=[2, 34671], truth_map=[14704], weights=[34671], scores=[34671])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency: 97.40%, Purity: 98.43%\n"
     ]
    }
   ],
   "source": [
    "score_cut = 0.8\n",
    "true = sample.y.sum()\n",
    "positive = (sample.scores > score_cut).sum()\n",
    "true_positive = ((sample.scores > score_cut) & sample.y).sum()\n",
    "\n",
    "eff = true_positive / true\n",
    "pur = true_positive / positive\n",
    "\n",
    "print(\"Efficiency: {:.2f}%, Purity: {:.2f}%\".format(eff*100, pur*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: NetworkX All Paths, Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to...\n",
    "# 1. Apply first score cut\n",
    "# 2. Using networkx, find all paths between all starting nodes and ending nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch geometric to convert to networkx\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply score cut\n",
    "edge_mask = sample.scores > score_cut\n",
    "sample.edge_index = sample.edge_index[:, edge_mask]\n",
    "sample.y = sample.y[edge_mask]\n",
    "sample.scores = sample.scores[edge_mask]\n",
    "\n",
    "# Convert to networkx graph\n",
    "G = to_networkx(sample, to_undirected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 278183 \n",
      "Number of edges: 13817 \n",
      "Number starting nodes: 264483 \n",
      "Number ending nodes: 264473\n"
     ]
    }
   ],
   "source": [
    "# Get some topline stats, starting nodes are nodes with no incoming edges, ending nodes are nodes with no outgoing edges\n",
    "print(f\"Number of nodes: {G.number_of_nodes()} \\nNumber of edges: {G.number_of_edges()} \\nNumber starting nodes: {len([n for n in G.nodes() if G.in_degree(n) == 0])} \\nNumber ending nodes: {len([n for n in G.nodes() if G.out_degree(n) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove isolated nodes\n",
    "G.remove_nodes_from(list(nx.isolates(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 15175 \n",
      "Number of edges: 13817 \n",
      "Number starting nodes: 1475 \n",
      "Number ending nodes: 1465\n"
     ]
    }
   ],
   "source": [
    "# Get some topline stats, starting nodes are nodes with no incoming edges, ending nodes are nodes with no outgoing edges\n",
    "print(f\"Number of nodes: {G.number_of_nodes()} \\nNumber of edges: {G.number_of_edges()} \\nNumber starting nodes: {len([n for n in G.nodes() if G.in_degree(n) == 0])} \\nNumber ending nodes: {len([n for n in G.nodes() if G.out_degree(n) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [00:19<00:00, 74.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "# Get all paths between starting and ending nodes\n",
    "paths = []\n",
    "for start in tqdm(starting_nodes):\n",
    "    paths.extend(\n",
    "        list(nx.all_simple_paths(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    )\n",
    "# Get the number of paths by expanding out the path list of lists of lists\n",
    "paths = list(chain.from_iterable(paths))\n",
    "num_paths = len(paths)\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [00:19<00:00, 75.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1523\n",
      "CPU times: user 19.7 s, sys: 30.1 ms, total: 19.7 s\n",
      "Wall time: 19.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "# Get shortest path between starting and ending nodes\n",
    "paths = []\n",
    "for start in tqdm(starting_nodes):\n",
    "    paths.extend(\n",
    "        list(nx.shortest_path(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    )\n",
    "\n",
    "# Get the number of paths\n",
    "num_paths = len(paths)\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [00:20<00:00, 72.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1523\n",
      "CPU times: user 20.5 s, sys: 0 ns, total: 20.5 s\n",
      "Wall time: 20.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "# Get shortest path between starting and ending nodes using the A star function\n",
    "paths = []\n",
    "for start in tqdm(starting_nodes):\n",
    "    paths.extend(\n",
    "        list(nx.astar_path(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    )\n",
    "\n",
    "# Get the number of paths\n",
    "num_paths = len(paths)\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [00:17<00:00, 85.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1523\n",
      "CPU times: user 17.2 s, sys: 21.7 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "# Get shortest path between starting and ending nodes using the A star function\n",
    "paths = []\n",
    "for start in tqdm(starting_nodes):\n",
    "    paths.extend(\n",
    "        list(nx.bidirectional_shortest_path(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    )\n",
    "\n",
    "# Get the number of paths\n",
    "num_paths = len(paths)\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Multithreaded NetworkX All Paths, Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to...\n",
    "# 1. Apply first score cut\n",
    "# 2. Using networkx, find all paths between all starting nodes and ending nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch geometric to convert to networkx\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply score cut\n",
    "edge_mask = sample.scores > score_cut\n",
    "sample.edge_index = sample.edge_index[:, edge_mask]\n",
    "sample.y = sample.y[edge_mask]\n",
    "sample.scores = sample.scores[edge_mask]\n",
    "\n",
    "# Convert to networkx graph\n",
    "G = to_networkx(sample, to_undirected=False)\n",
    "G.remove_nodes_from(list(nx.isolates(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 15175 \n",
      "Number of edges: 13817 \n",
      "Number starting nodes: 1475 \n",
      "Number ending nodes: 1465\n"
     ]
    }
   ],
   "source": [
    "# Get some topline stats, starting nodes are nodes with no incoming edges, ending nodes are nodes with no outgoing edges\n",
    "print(f\"Number of nodes: {G.number_of_nodes()} \\nNumber of edges: {G.number_of_edges()} \\nNumber starting nodes: {len([n for n in G.nodes() if G.in_degree(n) == 0])} \\nNumber ending nodes: {len([n for n in G.nodes() if G.out_degree(n) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shortest_paths(start):\n",
    "    return [\n",
    "        nx.shortest_path(G, start, end)\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_paths(start):\n",
    "    return list(chain.from_iterable([\n",
    "        list(nx.all_simple_paths(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [00:00<00:00, 1983.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1586\n",
      "CPU times: user 477 ms, sys: 459 ms, total: 936 ms\n",
      "Wall time: 1.52 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Do the above, but using tqdm pool\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "workers = 32\n",
    "\n",
    "paths = process_map(find_all_paths, starting_nodes, max_workers=workers, chunksize=1)\n",
    "\n",
    "# Get the number of paths\n",
    "num_paths = len(list(chain.from_iterable(paths)))\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths: 1586\n"
     ]
    }
   ],
   "source": [
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "workers = 32\n",
    "\n",
    "# Use process pool\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(workers) as p:\n",
    "    paths = p.map(find_all_paths, starting_nodes)\n",
    "\n",
    "# Get the number of paths\n",
    "num_paths = len(list(chain.from_iterable(paths)))\n",
    "print(f\"Number of paths: {num_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_paths(start, G, ending_nodes):\n",
    "    return list(chain.from_iterable([\n",
    "        list(nx.all_simple_paths(G, start, end))\n",
    "        for end in ending_nodes\n",
    "        if nx.has_path(G, start, end)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "starting_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "ending_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "\n",
    "workers = 32\n",
    "\n",
    "# Use process pool\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "find_all_paths_partial = partial(find_all_paths, G=G, ending_nodes=ending_nodes)\n",
    "\n",
    "with Pool(workers) as p:\n",
    "    paths = p.map(find_all_paths_partial, starting_nodes)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cugraph and use it in place of networkx\n",
    "import cugraph\n",
    "import cudf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that labels each node with the path it is in\n",
    "track_df = pd.DataFrame(\n",
    "    {\n",
    "        \"hit_id\": list(chain.from_iterable(paths)),\n",
    "        \"track_id\": list(chain.from_iterable([[i] * len(p) for i, p in enumerate(paths)])),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates on hit_id\n",
    "track_df = track_df.drop_duplicates(subset=\"hit_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_id = track_df.hit_id\n",
    "track_id = track_df.track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id_tensor = torch.ones(len(sample.x), dtype=torch.long) * -1\n",
    "track_id_tensor[hit_id.values] = torch.from_numpy(track_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.labels = track_id_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"track_building_eval.yaml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Wandb found, using WandbLogger\n",
      "WARNING:root:FRNN is not available, install it at https://github.com/murnanedaniel/FRNN. Using PyG radius instead.\n"
     ]
    }
   ],
   "source": [
    "from gnn4itk_cf.stages.track_building import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reconstruction_df(graph):\n",
    "    \"\"\"Load the reconstructed tracks from a file.\"\"\"\n",
    "    pids = torch.zeros(graph.hit_id.shape[0], dtype=torch.int64)\n",
    "    pids[graph.track_edges[0]] = graph.particle_id\n",
    "    pids[graph.track_edges[1]] = graph.particle_id\n",
    "\n",
    "    return pd.DataFrame({\"hit_id\": graph.hit_id, \"track_id\": graph.labels, \"particle_id\": pids})\n",
    "\n",
    "def load_particles_df(graph):\n",
    "    \"\"\"Load the particles from a file.\"\"\"\n",
    "    # Get the particle dataframe\n",
    "    particles_df = pd.DataFrame({\"particle_id\": graph.particle_id, \"pt\": graph.pt})\n",
    "\n",
    "    # Reduce to only unique particle_ids\n",
    "    particles_df = particles_df.drop_duplicates(subset=['particle_id'])\n",
    "\n",
    "    return particles_df\n",
    "\n",
    "def get_matching_df(reconstruction_df, min_track_length=1, min_particle_length=1):\n",
    "    \n",
    "    # Get track lengths\n",
    "    candidate_lengths = reconstruction_df.track_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"track_id\", \"track_id\": \"n_reco_hits\"})\n",
    "\n",
    "    # Get true track lengths\n",
    "    particle_lengths = reconstruction_df.drop_duplicates(subset=['hit_id']).particle_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"particle_id\", \"particle_id\": \"n_true_hits\"})\n",
    "\n",
    "    spacepoint_matching = reconstruction_df.groupby(['track_id', 'particle_id']).size()\\\n",
    "        .reset_index().rename(columns={0:\"n_shared\"})\n",
    "\n",
    "    spacepoint_matching = spacepoint_matching.merge(candidate_lengths, on=['track_id'], how='left')\n",
    "    spacepoint_matching = spacepoint_matching.merge(particle_lengths, on=['particle_id'], how='left')\n",
    "    # spacepoint_matching = spacepoint_matching.merge(particles_df, on=['particle_id'], how='left')\n",
    "\n",
    "    # Filter out tracks with too few shared spacepoints\n",
    "    spacepoint_matching[\"is_matchable\"] = spacepoint_matching.n_reco_hits >= min_track_length\n",
    "    spacepoint_matching[\"is_reconstructable\"] = spacepoint_matching.n_true_hits >= min_particle_length\n",
    "\n",
    "    return spacepoint_matching\n",
    "\n",
    "def calculate_matching_fraction(spacepoint_matching_df):\n",
    "    spacepoint_matching_df = spacepoint_matching_df.assign(\n",
    "        purity_reco=np.true_divide(spacepoint_matching_df.n_shared, spacepoint_matching_df.n_reco_hits))\n",
    "    spacepoint_matching_df = spacepoint_matching_df.assign(\n",
    "        eff_true = np.true_divide(spacepoint_matching_df.n_shared, spacepoint_matching_df.n_true_hits))\n",
    "\n",
    "    return spacepoint_matching_df\n",
    "\n",
    "def evaluate_labelled_graph(graph, matching_fraction=0.5, matching_style=\"ATLAS\", min_track_length=1, min_particle_length=1):\n",
    "\n",
    "    if matching_fraction < 0.5:\n",
    "        raise ValueError(\"Matching fraction must be >= 0.5\")\n",
    "\n",
    "    if matching_fraction == 0.5:\n",
    "        # Add a tiny bit of noise to the matching fraction to avoid double-matched tracks\n",
    "        matching_fraction += 0.00001\n",
    "\n",
    "    # Load the labelled graphs as reconstructed dataframes\n",
    "    reconstruction_df = load_reconstruction_df(graph)\n",
    "    particles_df = load_particles_df(graph)\n",
    "\n",
    "    # Get matching dataframe\n",
    "    matching_df = get_matching_df(reconstruction_df, particles_df, min_track_length=min_track_length, min_particle_length=min_particle_length) \n",
    "    matching_df[\"event_id\"] = int(graph.event_id)\n",
    "\n",
    "    # calculate matching fraction\n",
    "    matching_df = calculate_matching_fraction(matching_df)\n",
    "\n",
    "    # Run matching depending on the matching style\n",
    "    if matching_style == \"ATLAS\":\n",
    "        matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = matching_df.purity_reco >= matching_fraction\n",
    "    elif matching_style == \"one_way\":\n",
    "        matching_df[\"is_matched\"] = matching_df.purity_reco >= matching_fraction\n",
    "        matching_df[\"is_reconstructed\"] = matching_df.eff_true >= matching_fraction\n",
    "    elif matching_style == \"two_way\":\n",
    "        matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = (matching_df.purity_reco >= matching_fraction) & (matching_df.eff_true >= matching_fraction)\n",
    "\n",
    "    return matching_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "            min_particle_length=config[\"min_particle_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    logging.info(f\"Number of particles: {n_particles}\")\n",
    "    logging.info(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    logging.info(f\"Number of tracks: {n_tracks}\")\n",
    "    logging.info(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.92it/s]\n",
      "INFO:root:Number of reconstructed particles: 1308\n",
      "INFO:root:Number of particles: 1425\n",
      "INFO:root:Number of matched tracks: 1384\n",
      "INFO:root:Number of tracks: 1390\n",
      "INFO:root:Number of duplicate reconstructed particles: 75\n",
      "INFO:root:Efficiency: 0.918\n",
      "INFO:root:Fake rate: 0.004\n",
      "INFO:root:Duplication rate: 0.057\n"
     ]
    }
   ],
   "source": [
    "evaluate_labelled_graphs([sample], config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter to Handle Multiple Labels per Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reconstruction_df_new(graph, track_df):\n",
    "    \"\"\"Load the reconstructed tracks from a file.\"\"\"\n",
    "    particle_df = pd.DataFrame(\n",
    "        {\n",
    "            \"hit_id\": torch.flatten(graph.track_edges),\n",
    "            \"particle_id\": graph.particle_id.repeat(2),\n",
    "            \"pt\": graph.pt.repeat(2),\n",
    "            \"primary\": graph.primary.repeat(2),\n",
    "        }\n",
    "    )\n",
    "    particle_df = particle_df.drop_duplicates(subset=[\"hit_id\", \"particle_id\"])\n",
    "\n",
    "    return pd.merge(\n",
    "        track_df,\n",
    "        particle_df,\n",
    "        on=\"hit_id\",\n",
    "        how=\"outer\",\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_df_new(reconstruction_df, min_track_length=1, min_particle_length=1):\n",
    "    \n",
    "    # Get track lengths\n",
    "    candidate_lengths = reconstruction_df.drop_duplicates(subset=['hit_id', 'particle_id']).track_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"track_id\", \"track_id\": \"n_reco_hits\"})\n",
    "\n",
    "    # Get true track lengths\n",
    "    particle_lengths = reconstruction_df.drop_duplicates(subset=['hit_id', 'track_id']).particle_id.value_counts(sort=False)\\\n",
    "        .reset_index().rename(\n",
    "            columns={\"index\":\"particle_id\", \"particle_id\": \"n_true_hits\"})\n",
    "\n",
    "    spacepoint_matching = reconstruction_df.groupby(['track_id', 'particle_id']).size()\\\n",
    "        .reset_index().rename(columns={0:\"n_shared\"})\n",
    "\n",
    "    spacepoint_matching = spacepoint_matching.merge(candidate_lengths, on=['track_id'], how='left')\n",
    "    spacepoint_matching = spacepoint_matching.merge(particle_lengths, on=['particle_id'], how='left')\n",
    "    # spacepoint_matching = spacepoint_matching.merge(particles_df, on=['particle_id'], how='left')\n",
    "\n",
    "    # Filter out tracks with too few shared spacepoints\n",
    "    spacepoint_matching[\"is_matchable\"] = spacepoint_matching.n_reco_hits >= min_track_length\n",
    "    spacepoint_matching[\"is_reconstructable\"] = spacepoint_matching.n_true_hits >= min_particle_length\n",
    "\n",
    "    return spacepoint_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of reconstructed particles: 1307\n",
      "INFO:root:Number of particles: 1360\n",
      "INFO:root:Number of matched tracks: 1383\n",
      "INFO:root:Number of tracks: 1389\n",
      "INFO:root:Number of duplicate reconstructed particles: 82\n",
      "INFO:root:Efficiency: 0.961\n",
      "INFO:root:Fake rate: 0.004\n",
      "INFO:root:Duplication rate: 0.063\n"
     ]
    }
   ],
   "source": [
    "graph = sample\n",
    "min_track_length, min_particle_length = 3, 3\n",
    "matching_fraction = 0.5\n",
    "if matching_fraction == 0.5:\n",
    "    # Add a tiny bit of noise to the matching fraction to avoid double-matched tracks\n",
    "    matching_fraction += 0.00001\n",
    "\n",
    "reconstruction_df = load_reconstruction_df_new(graph, track_df)\n",
    "# particles_df = load_particles_df(graph)\n",
    "\n",
    "# Get matching dataframe\n",
    "matching_df = get_matching_df_new(reconstruction_df, min_track_length=min_track_length, min_particle_length=min_particle_length) \n",
    "matching_df[\"event_id\"] = int(graph.event_id)\n",
    "\n",
    "# calculate matching fraction\n",
    "matching_df = calculate_matching_fraction(matching_df)\n",
    "\n",
    "matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = matching_df.purity_reco >= matching_fraction\n",
    "\n",
    "particles = matching_df[matching_df[\"is_reconstructable\"]]\n",
    "reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "tracks = matching_df[matching_df[\"is_matchable\"]]\n",
    "matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "logging.info(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "logging.info(f\"Number of particles: {n_particles}\")\n",
    "logging.info(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "logging.info(f\"Number of tracks: {n_tracks}\")\n",
    "logging.info(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "# Plot the results across pT and eta\n",
    "eff = n_reconstructed_particles / n_particles\n",
    "fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "logging.info(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of reconstructed particles: 1307\n",
      "INFO:root:Number of particles: 1360\n",
      "INFO:root:Number of matched tracks: 1383\n",
      "INFO:root:Number of tracks: 1389\n",
      "INFO:root:Number of duplicate reconstructed particles: 82\n",
      "INFO:root:Efficiency: 0.961\n",
      "INFO:root:Fake rate: 0.004\n",
      "INFO:root:Duplication rate: 0.063\n"
     ]
    }
   ],
   "source": [
    "graph = sample\n",
    "min_track_length, min_particle_length = 3, 3\n",
    "matching_fraction = 0.5\n",
    "if matching_fraction == 0.5:\n",
    "    # Add a tiny bit of noise to the matching fraction to avoid double-matched tracks\n",
    "    matching_fraction += 0.00001\n",
    "\n",
    "reconstruction_df = load_reconstruction_df_new(graph, track_df)\n",
    "# particles_df = load_particles_df(graph)\n",
    "\n",
    "# Get matching dataframe\n",
    "matching_df = get_matching_df_new(reconstruction_df, min_track_length=min_track_length, min_particle_length=min_particle_length) \n",
    "matching_df[\"event_id\"] = int(graph.event_id)\n",
    "\n",
    "# calculate matching fraction\n",
    "matching_df = calculate_matching_fraction(matching_df)\n",
    "\n",
    "matching_df[\"is_matched\"] = matching_df[\"is_reconstructed\"] = matching_df.purity_reco >= matching_fraction\n",
    "\n",
    "particles = matching_df[matching_df[\"is_reconstructable\"]]\n",
    "reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "tracks = matching_df[matching_df[\"is_matchable\"]]\n",
    "matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "logging.info(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "logging.info(f\"Number of particles: {n_particles}\")\n",
    "logging.info(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "logging.info(f\"Number of tracks: {n_tracks}\")\n",
    "logging.info(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "# Plot the results across pT and eta\n",
    "eff = n_reconstructed_particles / n_particles\n",
    "fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "logging.info(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4itk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab48adb41b8b8044ca3e1b10285c01a4f474a98b0c8dc3ffd314e85581ffd197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
